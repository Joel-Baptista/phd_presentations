% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,width=70,height=100,scale=0.963]{beamerposter}
% \usepackage[size=custom,width=70,height=100,scale=0.9]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
% \usepackage{pgfplotstable}
% \usepackage{soul}

\usepackage{kinematikz} %requires TeX Live 2022!!
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usetikzlibrary{positioning,backgrounds,patterns}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{shapes.geometric}
% \sisetup{ detect-family = true, } %to allow sans-serif in math mode units
\usepackage{ifthen}
\usetikzlibrary{matrix,calc}

\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{fillbetween} 
\usepgfplotslibrary{units}

\newcommand{\graphLineWidth}{0.4mm}
\newcommand{\pullColor}{red}
\newcommand{\pushColor}{blue}
\newcommand{\shakeColor}{green}
\newcommand{\twistColor}{orange}
\newcommand{\actionOpacity}{0.10}
% A base color for the confusion matrix
\definecolor{cfcolor}{HTML}{042D6E}
\usepackage{svg}

%Adjust these definitions for adequate consistency in the text. This is an example. You can adjust for better typesetting!
\newcommand{\pull}{\textsc{pull}}
\newcommand{\push}{\textsc{push}}
\newcommand{\shake}{\textsc{shake}}
\newcommand{\twist}{\textsc{twist}}

\let\comment\undefined % because someone else defines it (amsmath, verbatim, anyone else ...?)
% \usepackage[commandnameprefix=ifneeded]{changes}
% \usepackage[final]{changes} % for the final acceptance of all changes
% \usepackage{enumitem}
\tikzset{
   vsblack/.style={
   black, %force black color override the elsevier template inside tikz figures and plots. Comment this line not to override!
   }, 
}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{Domain Adaptation with Contrastive Simultaneous Multi-loss
Training for Hand Gesture Recognition}

\author{Joel Baptista \inst{1} \and VÃ­tor Santos \inst{1} \and Filipe Silva \inst{1} \and Diogo Pinho \inst{2}}

\institute[shortinst]{\inst{1} Institute of Electronics and Informatics Engineering of Aveiro (IEETA) \samelineand \\ \inst{2} Bosch Termotecnologia, S. A.  }

% ====================
% Footer (optional)
% ====================

\footercontent{
  \href{https://doi.org/10.3390/s23063332}{https://doi.org/10.3390/s23063332} \hfill
  \hfill
  \href{mailto:joelbaptista@ua.pt}{joelbaptista@ua.pt}}
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
\logoright{\includegraphics[height=10cm, trim={0 0.5cm 0 0}, clip]{img/Ciencia_2024_Horizontal_Branco.png}}
% \logoright{\includegraphics[height=1cm]{img/Ciencia_2024_Horizontal_Branco.png}}
\logoleft{\includegraphics[height=9cm, trim={0 0.0 12cm 0}, clip]{img/ieeta.png}}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Introduction}


    Hand gestures are an important aspect of human communication, serving several purposes 
    such as enhancing spoken messages, signaling intentions or expressing emotions. 
    Driven by technological advances, the process of classifying meaningful hand gestures, 
    known as Hand Gesture Recognition (HGR), has received increasing attention in recent years.
    
    The major application areas of gesture recognition include sign language translation, 
    human-machine interaction, medical rehabilitation, and virtual reality. HGR systems also 
    target robotic applications using a variety of input devices, among which stands out color
    cameras, depth sensors or gloves with embedded sensors. In this context, the ability of 
    robots to recognize hand gestures
    seems very promising for progress in Human-Robot Collaboration (HRC), 
    as these gestures are simple and intuitive for a human partner to produce. This, in turn, allows 
    humans and robots to coexist and cooperate, improving task efficiency and safety.


    However, HGR is an inherently challenging task due to the complex, non-rigid properties of the hand, 
    such as its shape, color, and orientation. Vision-based HGR systems must also be robust to variations 
    in lighting conditions, cluttered environments, complex backgrounds, and occlusions. In reality, the
    assumption that the training and test datasets are drawn from the same distribution rarely holds in
    practical situations due to domain shifts.
    
    To tackle this issue, this paper proposes a domain adaptation technique for hand gesture recognition in 
    human-robot collaboration scenarios. The proposed approach is based on a two-headed 
    deep architecture that simultaneously adopts cross-entropy and a contrastive loss from
    different network branches to classify hand gestures.

  \end{block}

  \begin{block}{Objectives}

    The main goal is to shed light on the impact of supervised contrastive learning (SCL) 
    on the generalization ability of a trained deep model in gesture classification when faced with a distribution shift.
    For this purpose, the study:

    \begin{itemize}
      \item Contributes with a new RGB annotated dataset of hand gestures, with a complex background 
      and multiple subjects.
      \item Compares the results from the proposed approach against baselines.
    \end{itemize}

  \end{block}

  \begin{block}{Dataset Description}

    This implementation uses four hand gesture
    classes inspired by American Sign Language; the symbols chosen are the "A", "F", "L" and 
    "Y" signs. These signals have the advantage of being well known symbols with already real
    world applications, implying that they are easy to use. These specific signs were chosen
    also because they are relatively distinct. 

    To test the degree of generalization of the proposed method, two datasets were recorded.
    The first dataset was used to train the classification model. This dataset was recorded by one subject, 
    which can be seen in Figure \autoref{fig:gestures_train}.
    
    \begin{figure}[!ht]
      \centering
      \def\vsTW{0.2\textwidth}  %was 0.15\textwidth
      \setlength{\tabcolsep}{2pt} %to make a shorter separation
      \newcommand{\vsTE}[1]{\includegraphics[width=\vsTW]{gestures/#1.png}}
      \begin{tabular}{cccc}
      A & F & L & Y \\
      \vsTE{A1} & \vsTE{F1} & \vsTE{L1} & \vsTE{Y1} \\
      \vsTE{A2} & \vsTE{F2} & \vsTE{L2} & \vsTE{Y2} \\
      \end{tabular}
      \caption{Examples of the training dataset of the four hand gesture classes in the unstructured environment and complex background.
      }
      \label{fig:gestures_train}
    \end{figure}


  \end{block}


\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \vspace{1.5cm}
  \justify

  The second dataset is used to test the model. This multi-user test dataset was recorded with three subjects
  who were not included in the training dataset. The dataset was recorded on a different day and at a 
  different time of day, resulting in variation in luminosity. Figure 2 shows 
  some samples that constitute the multi-user test dataset.
  
  \begin{figure}[!ht]
    \justify
    \centering
    \def\vsTW{0.2\textwidth}  %was 0.15\textwidth
    \setlength{\tabcolsep}{2pt} %to make a shorter separation
    \newcommand{\vsTE}[1]{\includegraphics[width=\vsTW]{gestures/#1.png}}
    \begin{tabular}{cccc}
    A & F & L & Y \\
    \vsTE{MU_A2} & \vsTE{MU_F2} & \vsTE{MU_L2} & \vsTE{MU_Y2} \\
    \vsTE{MU_A3} & \vsTE{MU_F3} & \vsTE{MU_L3} & \vsTE{MU_Y3} \\
    \end{tabular}
    \caption{Examples of the test dataset with three different subjects and acquired in a different time of day in relation to the training dataset.\label{fig:gestures_test}}
  \end{figure}
  
  Table \autoref{tab:dataset} shows the distribution of samples among all classes. Although the dataset is small
  when compared to the large-scale datasets, it has a distribution of samples per class similar to other 
  static hand gesture datasets used in HGR. Additionally, we apply online data augmentation in the training phase that further help
  compensate for the reduced number of samples. 
 
  \begin{table}[!ht] 
    \centering
    %\newcolumntype{C}{>{\centering\arraybackslash}X}
    \begin{tabular}{lccccc}
      \toprule
      \textbf{Dataset}	& \textbf{A}	& \textbf{F} & \textbf{L} & \textbf{Y} & \textbf{Total}\\
      \midrule
      Training dataset & \num{6430} & \num{6148} & \num{5989} & \num{6044} & \textbf{\num{24611}}\\
      
      Multi-user test dataset & \num{4183} & \num{4277} & \num{4276} & \num{4316} & \textbf{\num{17051}}\\
      \bottomrule
    \end{tabular}
    \caption{Number of images, per class, of the training dataset and the multi-user test dataset. The training dataset includes one subject and the test dataset includes three subjects.\label{tab:dataset}}
    % \noindent{\footnotesize{\textsuperscript{1} Tables may have a footer.}}
  \end{table}

  Both of the datasets were recorded in the collaborative cell located in 
  the Laboratory of Automation and Robotics at the University of Aveiro.

  \begin{block}{Methodology}

    This section describes the proposed contrastive domain adaptation technique for HGR. 
    Our goal is to train a model on the source domain and, then, use it to make
    predictions on the target domain that has different characteristics, namely, different subjects
    and illumination conditions. We compare this approach with traditional transfer learning methods, that 
    consists of the Inception-v3 pre-trained model that is repurposed
    for the specific hand gesture classification task involving four classes.

    SCL has been shown to be effective for domain adaptation, 
    because it can help the model to learn features that are invariant to domain shifts.
    
    The idea behind the proposed contrastive domain adaptation technique is to use a network that 
    branches twice after the encoder model (dual-branch head), allowing to train the representation 
    model and the classification model simultaneously. Figure \autoref{fig:multi_loss} shows the model architecture.

    \begin{figure}[!ht]
      \centering
      % \input{imgs/multi_loss.tikz}
      \includegraphics[width=\textwidth]{img/architecture.png}
      \caption{Model's architecture. The losses used are the contrastive loss and the Cross Entropy Loss. \label{fig:multi_loss}}
    \end{figure} 

  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{exampleblock}{Contrastive Learning}

    In Contrastive Learning we use a contrastive loss, defined by:

    \begin{equation}
      C_{loss} = \sum_{i \in B} \frac{-1}{p} \sum_{j \in P(i)} \log \frac{e^{(v_i \cdot v_j) / \tau}}{\sum\limits_{a \in A(i)} e^{(v_i \cdot v_a) / \tau}}
      \label{eq:con_loss}\text{,}
    \end{equation}

    This loss function encourages the base model to produce
    similar feature vectors in the same class and dissimilar feature vectors in 
    different classes.

    \heading{Variables}

    Each batch $B$ has $v_i$ feature vectors. The set $P(i)$ 
    represents the indexes of the positive samples $j$ in relation to an anchor 
    sample $i$, and it has a size of $p$. A sample is classified as positive when 
    it belongs to the same class as the anchor. The set $A(i)$ includes all
    the indexes of $B$ except $i$. The exponents exhibit the dot product between 
    two feature vectors divided by a scalar temperature parameter $\tau$.

  \end{exampleblock}
  \begin{block}{Results}

    \def\globalCMscale{1.9}
    % \begin{figure}[!ht]
    %   \centering
    %   \input{cm/cm_normal.tikz}%
    %   % \input{cm/cm_multi_stage.tikz}%
    %   \input{cm/cm_multi_loss.tikz}
    %   \caption{Confusion Matrices of results, in percentage (\%), obtained by training all both models with the single user dataset and testing them with the multi-user dataset with different persons not present in the training dataset; the models are: SLT (left), MSMLT (middle) and SMLT (right).}
    %   \label{fig:cm_normal}
    % \end{figure}

    \begin{figure}[!ht]
      \justify
      \centering
      \def\vsTW{0.2\textwidth}  %was 0.15\textwidth
      \setlength{\tabcolsep}{2pt} %to make a shorter separation
      \newcommand{\vsTE}[1]{\includegraphics[width=\vsTW]{gestures/#1.png}}
      \begin{tabular}{cc}
      \hspace{25mm} Baseline & \hspace{17mm} Our Approach \\
      \input{cm/cm_normal.tikz} & \input{cm/cm_multi_loss.tikz} \\
      \end{tabular}
      \caption{Examples of the test dataset with three different subjects and acquired in a different time of day in relation to the training dataset.\label{fig:gestures_test}}
    \end{figure}

    \begin{table}[!ht] 
      \centering
      %\newcolumntype{C}{>{\centering\arraybackslash}X}
      \begin{tabular}{lcccc}
        \toprule
        \textbf{Model}	& \textbf{Accuracy}	& \textbf{Recall} & \textbf{Precision} & \textbf{F1}\\
        \midrule
        Baseline & 84.23	& 84.15 & 86.82 & 84.07\\
        Our approach & \textbf{90.03} & \textbf{89.97} & \textbf{90.96} & \textbf{90.12} \\
        \bottomrule
      \end{tabular}
      \caption{Evaluation metrics for testing the both approaches in the multi-user test dataset. These values are the average of the metrics calculated for each class.\label{tab:metrics}}
      % \noindent{\footnotesize{\textsuperscript{1} Tables may have a footer.}}
      \end{table}

  \end{block}

  \begin{block}{Conclusions}

    The focus of this study was the generalization capacity of the model, which was tested using the multi-user
    test dataset. In this testing phase, the results show that joining cross entropy loss and contrastive loss in a
    multi-loss training approach helps the model reach higher accuracy. In fact, this approach
    performed an increase of 6\% in the accuracy of the model, compared to the traditional transfer learning
    method of training the model only with the CEL. This shows that contrastive learning is
    focused on learning task-specific and invariant features, being more effective to deal with the domain shift
    problem.

  \end{block}


  \begin{block}{References}

    % \nocite{*}
    % \footnotesize{\bibliographystyle{plain}\bibliography{poster}}
    % \input{bib}
    \footnotesize

    Rato, D., Oliveira, M., Santos, V., Gomes, M. and Sappa, A. (2022) 'A sensor-to-pattern calibration framework for multi-modal industrial collaborative cells', Journal of Manufacturing Systems, 64, pp. 497-507. doi: https://doi.org/10.1016/j.jmsy.2022.07.006.

    Castro, A., Silva, F. and Santos, V. (2021) 'Trends of Human-Robot Collaboration in Industry Contexts: Handover, Learning, and Metrics', Sensors, 21(12), 4113. doi: 10.3390/s21124113.

    Castro, A., Baptista, J., Silva, F. and Santos, V. (2023) 'Classification of handover interaction primitives in a COBOTâhuman context with a deep neural network', Journal of Manufacturing Systems, 68, pp. 289-302. doi: https://doi.org/10.1016/j.jmsy.2023.03.010.

    Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C. and Krishnan, D. (2020) 'Supervised Contrastive Learning', CoRR, 2004.11362. Available at: https://arxiv.org/abs/2004.11362 (Accessed: 19 June 2024).


  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
